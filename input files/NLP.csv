Area,Parent Task,Task,Dataset,Best method name,Primary metric,earliest reported value,latest reported value,earliest reported value scaled,earliest  Percentile ( low/ high),latest reported  value scaled,Latest Percentile  ( low/ high),Bucket
Natural Language Processing,Language Modelling,Language Modelling,Penn Treebank (Word Level),GPT-2,Test perplexity,65.4,35.76,19.4,Low,10.75,Low,1
Natural Language Processing,Language Modelling,Language Modelling,One Billion Word,Transformer-XL Large,PPL,51.3,21.8,15.14,Low,6.47,Low,1
Natural Language Processing,Language Modelling,Language Modelling,WikiText-103,GPT-2,Test perplexity,40.8,17.48,11.96,Low,5.15,Low,1
Natural Language Processing,Language Modelling,Language Modelling,WikiText-2,GPT-2,Test perplexity,52,18.34,15.35,Low,5.41,Low,1
Natural Language Processing,Language Modelling,Language Modelling,Hutter Prize,24-layer Transformer-XL,Bit per Character (BPC),1.27,0.99,0.02,Low,0.09,Low,1
Natural Language Processing,Language Modelling,Language Modelling,enwiki8,GPT-2,Bit per Character (BPC),1.27,0.93,0.02,Low,0.07,Low,1
Natural Language Processing,Language Modelling,Language Modelling,Text8,GPT-2,Bit per Character (BPC),1.36,0.98,0.04,Low,0.09,Low,1
Natural Language Processing,Language Modelling,Language Modelling,Penn Treebank (Character Level),Trellis Network,Bit per Character (BPC),1.219,1.158,0,Low,0.14,Low,1
Natural Language Processing,Question Answering,Question Answering,AI2 Kaggle Dataset,IR Baseline,P@1,47.2,47.2,13.9,Low,14.26,Low,1
Natural Language Processing,Question Answering,Question Answering,COMPLEXQUESTIONS,WebQA,F1,53.6,53.6,15.83,Low,16.23,Low,1
Natural Language Processing,Sentiment Analysis,Sentiment Analysis,Yelp Binary classification,ULMFiT,Error,4.88,2.16,1.11,Low,0.45,Low,1
Natural Language Processing,Sentiment Analysis,Sentiment Analysis,Yelp Fine-grained classification,ULMFiT,Error,37.95,29.98,11.1,Low,8.98,Low,1
Natural Language Processing,Text Classification,Text Classification,AG News,L MIXED,Error,9.51,4.95,2.51,Low,1.3,Low,1
Natural Language Processing,Text Classification,Text Classification,DBpedia,L MIXED,Error,1.55,0.7,0.1,Low,0,Low,1
Natural Language Processing,Text Classification,Text Classification,TREC-6,USE_T+CNN,Error,4,1.93,0.84,Low,0.38,Low,1
Natural Language Processing,Text Classification,Text Classification,TREC-50,Rules,Error,2.8,2.8,0.48,Low,0.64,Low,1
Natural Language Processing,Text Generation,Text Generation,Yahoo Questions,Aggressive VAE,NLL,332.1,326.7,100,High,100,High,4
Natural Language Processing,Word Sense Disambiguation,Word Sense Disambiguation,Supervised:,MFS baseline,Senseval 2,72,71.6,21.39,Low,21.75,Low,1
Natural Language Processing,Machine Translation,Unsupervised Machine Translation,WMT2016 English-Romanian,MLM pretraining for encoder and decoder,BLEU,33.3,33.3,9.7,Low,10,Low,1
Natural Language Processing,Named Entity Recognition (NER),Nested Named Entity Recognition,ACE 2004,Neural transition-based model,F1,73.3,73.3,21.78,Low,22.27,Low,1
Natural Language Processing,Nested Mention Recognition,Nested Mention Recognition,ACE 2004,Neural transition-based model,F1,73.3,73.3,21.78,Low,22.27,Low,1
Natural Language Processing,Nested Mention Recognition,Nested Mention Recognition,ACE 2005,Neural transition-based model,F1,73,73,21.69,Low,22.18,Low,1
Natural Language Processing,Machine Translation,Machine Translation,WMT2014 English-French,Transformer Big + BT,BLEU score,34.54,45.6,34.72,Low,45.86,Low,1
Natural Language Processing,Machine Translation,Machine Translation,WMT2014 English-German,DeepL,BLEU score,20.7,29.7,20.75,Low,29.81,Low,1
Natural Language Processing,Machine Translation,Machine Translation,IWSLT2015 German-English,Transformer,BLEU score,28.53,34.44,28.65,Low,34.59,Low,1
Natural Language Processing,Machine Translation,Machine Translation,WMT2016 English-Romanian,ConvS2S BPE40k,BLEU score,28.1,29.88,28.22,Low,29.99,Low,1
Natural Language Processing,Machine Translation,Machine Translation,IWSLT2015 English-German,Transformer,BLEU score,25.04,28.23,25.13,Low,28.32,Low,1
Natural Language Processing,Machine Translation,Machine Translation,WMT2015 English-German,ByteNet,BLEU score,22.8,26.26,22.87,Low,26.33,Low,1
Natural Language Processing,Machine Translation,Machine Translation,WMT2016 English-German,Attentional encoder-decoder + BPE,BLEU score,34.2,34.2,34.38,Low,34.35,Low,1
Natural Language Processing,Machine Translation,Machine Translation,WMT2016 German-English,Attentional encoder-decoder + BPE,BLEU score,38.6,38.6,38.82,Low,38.79,Low,1
Natural Language Processing,Machine Translation,Machine Translation,WMT2016 English-Russian,Attentional encoder-decoder + BPE,BLEU score,26,26,26.1,Low,26.07,Low,1
Natural Language Processing,Machine Translation,Machine Translation,WMT2016 Romanian-English,MLM pretraining,BLEU score,33.3,35.3,33.47,Low,35.46,Low,1
Natural Language Processing,Machine Translation,Machine Translation,WMT2014 German-English,Denoising autoencoders (non-autoregressive),BLEU score,23.2,25.43,23.27,Low,25.49,Low,1
Natural Language Processing,Machine Translation,Machine Translation,WMT2016 English-Czech,Attentional encoder-decoder + BPE,BLEU score,25.8,25.8,25.9,Low,25.87,Low,1
Natural Language Processing,Machine Translation,Machine Translation,WMT 2014 EN-DE,universal transformer base,BLEU,28.9,28.9,29.03,Low,29,Low,1
Natural Language Processing,Machine Translation,Machine Translation,IWSLT2015 Thai-English,Seq-KD + Seq-Inter + Word-KD,BLEU score,14.2,14.2,14.18,Low,14.15,Low,1
Natural Language Processing,Machine Translation,Machine Translation,WMT2015 English-Russian,C2-50k Segmentation,BLEU score,20.9,20.9,20.95,Low,20.92,Low,1
Natural Language Processing,Machine Translation,Machine Translation,WMT2016 Czech-English,Attentional encoder-decoder + BPE,BLEU score,31.4,31.4,31.55,Low,31.52,Low,1
Natural Language Processing,Machine Translation,Machine Translation,WMT2014 French-English,SMT + iterative backtranslation (unsupervised),BLEU score,25.87,25.87,25.97,Low,25.94,Low,1
Natural Language Processing,Machine Translation,Machine Translation,WMT2016 Russian-English,Attentional encoder-decoder + BPE,BLEU score,28,28,28.12,Low,28.09,Low,1
Natural Language Processing,Language Modelling,Language Modelling,Sequential MNIST,Trellis Network,Accuracy,99.2,99.2,100,High,100,High,4
Natural Language Processing,Question Answering,Question Answering,SQuAD1.1,BERT (ensemble),EM,67.9,87.43,68.4,High,88.11,High,4
Natural Language Processing,Question Answering,Question Answering,SQuAD2.0,BERT + DAE + AoA (ensemble),EM,70.3,87.15,70.82,High,87.83,High,4
Natural Language Processing,Question Answering,Question Answering,CNN / Daily Mail,GA+MAGE (32),CNN,69.4,78.6,69.91,High,79.19,High,4
Natural Language Processing,Question Answering,Question Answering,WikiQA,HyperQA,MAP,0.6,0.71,0.45,Low,0.53,Low,1
Natural Language Processing,Question Answering,Question Answering,bAbi,QRN,Accuracy (trained on 10k),0.93,1,0.79,Low,0.82,Low,1
Natural Language Processing,Question Answering,Question Answering,Children's Book Test,GPT-2,Accuracy-CN,0.69,0.93,0.55,Low,0.75,Low,1
Natural Language Processing,Question Answering,Question Answering,CoQA,BERT Large Augmented (single model),In-domain,67,82.5,67.49,High,83.13,High,4
Natural Language Processing,Question Answering,Question Answering,QASent,Attentive LSTM,MAP,0.68,0.73,0.54,Low,0.55,Low,1
Natural Language Processing,Question Answering,Question Answering,NarrativeQA,ConZNet,Rouge-L,36.74,46.67,36.94,Low,46.94,Low,1
Natural Language Processing,Question Answering,Question Answering,SemEvalCQA,HyperQA,P@1,0.75,0.81,0.61,Low,0.63,Low,1
Natural Language Processing,Question Answering,Question Answering,YahooCQA,HyperQA,P@1,0.57,0.68,0.42,Low,0.49,Low,1
Natural Language Processing,Question Answering,Question Answering,TriviaQA,MemoReader,EM,46.94,67.21,47.24,High,67.69,High,4
Natural Language Processing,Question Answering,Question Answering,MS MARCO,Masque Q&A Style,Rouge-L,23.96,52.2,24.04,Low,52.53,High,2
Natural Language Processing,Question Answering,Question Answering,NewsQA,DecaProp,F1,56.1,66.3,56.49,High,66.77,High,4
Natural Language Processing,Question Answering,Question Answering,TrecQA,HyperQA,MAP,0.71,0.77,0.57,Low,0.59,Low,1
Natural Language Processing,Question Answering,Question Answering,WikiHop,CFC,Test,42.9,70.6,43.16,Low,71.11,High,2
Natural Language Processing,Question Answering,Question Answering,Story Cloze Test,Finetuned Transformer LM,Accuracy,78.7,78.7,79.3,High,79.3,High,4
Natural Language Processing,Question Answering,Question Answering,WebQuestions,Memory Networks (ensemble),F1,0.3,0.42,0.15,Low,0.23,Low,1
Natural Language Processing,Question Answering,Question Answering,CliCR,Gated-Attention Reader,F1,33.9,33.9,34.07,Low,34.05,Low,1
Natural Language Processing,Question Answering,Question Answering,RACE,Finetuned Transformer LM,RACE-m,60.2,60.2,60.63,High,60.61,High,4
Natural Language Processing,Question Answering,Question Answering,Reverb,Weakly Supervised Embeddings,Accuracy,0.73,0.73,0.59,Low,0.55,Low,1
Natural Language Processing,Question Answering,Question Answering,MCTest-500,Parallel-Hierarchical,Accuracy,0.71,0.71,0.57,Low,0.53,Low,1
Natural Language Processing,Question Answering,Question Answering,Quora Question Pairs,BERT (single model),Accuracy,0.72,0.72,0.58,Low,0.54,Low,1
Natural Language Processing,Question Answering,Question Answering,QuAC,FlowQA (single model),F1,64.1,64.1,64.56,High,64.55,High,4
Natural Language Processing,Question Answering,Question Answering,Natural Questions,BERT-joint,F1 (Long),66.2,66.2,66.68,High,66.67,High,4
Natural Language Processing,Question Answering,Question Answering,SimpleQuestions,Memory Networks (ensemble),F1,0.64,0.64,0.49,Low,0.45,Low,1
Natural Language Processing,Question Answering,Question Answering,MCTest-160,"syntax, frame, coreference, and word embedding features",Accuracy,0.75,0.75,0.61,Low,0.57,Low,1
Natural Language Processing,Sentiment Analysis,Sentiment Analysis,SST-2 Binary classification,MT-DNN,Accuracy,85.4,95.6,86.07,High,96.36,High,4
Natural Language Processing,Sentiment Analysis,Sentiment Analysis,SST-5 Fine-grained classification,EDD-LG (shared),Accuracy,45.7,64.4,45.99,High,64.85,High,4
Natural Language Processing,Sentiment Analysis,Sentiment Analysis,IMDb,ULMFiT,Accuracy,92.33,95.4,93.06,High,96.16,High,4
Natural Language Processing,Sentiment Analysis,Sentiment Analysis,MR,MEAN,Accuracy,78.26,84.5,78.86,High,85.15,High,4
Natural Language Processing,Sentiment Analysis,Sentiment Analysis,Amazon Review Full,DRNN,Accuracy,60.2,64.43,60.63,High,64.88,High,4
Natural Language Processing,Sentiment Analysis,Sentiment Analysis,Amazon Review Polarity,DRNN,Accuracy,94.6,96.49,95.36,High,97.26,High,4
Natural Language Processing,Sentiment Analysis,Sentiment Analysis,Multi-Domain Sentiment Dataset,Distributional Correspondence Indexing,DVD,75.4,81,75.97,High,81.62,High,4
Natural Language Processing,Sentiment Analysis,Sentiment Analysis,CR,Block-sparse LSTM,Accuracy,92.2,92.2,92.93,High,92.93,High,4
Natural Language Processing,Sentiment Analysis,Sentiment Analysis,SemEval,LSTMs+CNNs ensemble with multiple conv. ops ,F1-score,0.69,0.69,0.55,Low,0.5,Low,1
Natural Language Processing,Sentiment Analysis,Sentiment Analysis,MPQA,USE_T+DAN (w2v w.e.) ,Accuracy,88.14,88.14,88.83,High,88.83,High,4
Natural Language Processing,Sentiment Analysis,Sentiment Analysis,Sogou News,fastText,Accuracy,96.8,96.8,97.58,High,97.58,High,4
Natural Language Processing,Text Classification,Text Classification,Yahoo! Answers,DRNN,Accuracy,72.3,76.26,72.84,High,76.83,High,4
Natural Language Processing,Text Classification,Text Classification,Ohsumed,SGCN,Accuracy,36.2,68.5,36.4,Low,68.99,High,2
Natural Language Processing,Text Classification,Text Classification,R52,SGCN,Accuracy,93.56,94,94.31,High,94.75,High,4
Natural Language Processing,Text Classification,Text Classification,20NEWS,SGCN,Accuracy,86.34,88.5,87.02,High,89.19,High,4
Natural Language Processing,Text Classification,Text Classification,R8,SGCN,Accuracy,97.07,97.2,97.85,High,97.98,High,4
Natural Language Processing,Text Classification,Text Classification,Sogou News,CCCapsNet,Accuracy,97.25,97.25,98.03,High,98.03,High,4
Natural Language Processing,Text Classification,Text Classification,IMDb,L MIXED,Accuracy,95.68,95.68,96.45,High,96.44,High,4
Natural Language Processing,Natural Language Inference,Natural Language Inference,SNLI,MT-DNN,% Test Accuracy,78.2,91.1,78.8,High,91.82,High,4
Natural Language Processing,Natural Language Inference,Natural Language Inference,MultiNLI,MT-DNN,Matched,71.4,86.7,71.93,High,87.38,High,4
Natural Language Processing,Natural Language Inference,Natural Language Inference,SciTail,MT-DNN,Accuracy,83.3,94.1,83.95,High,94.85,High,4
Natural Language Processing,Natural Language Inference,Natural Language Inference,V-SNLI,V-BiMPM,Accuracy,86.99,86.99,87.67,High,87.67,High,4
Natural Language Processing,Natural Language Inference,Natural Language Inference,Quora Question Pairs,aESIM,Accuracy,88.01,88.01,88.7,High,88.7,High,4
Natural Language Processing,Named Entity Recognition (NER),Named Entity Recognition (NER),CoNLL 2003 (English),CNN Large + fine-tune,F1,91.21,93.5,91.93,High,94.24,High,4
Natural Language Processing,Named Entity Recognition (NER),Named Entity Recognition (NER),Ontonotes v5 (English),Flair embeddings,F1,86.99,89.71,87.67,High,90.42,High,4
Natural Language Processing,Named Entity Recognition (NER),Named Entity Recognition (NER),JNLPBA,CollaboNet,F1,78.58,78.58,79.18,High,79.17,High,4
Natural Language Processing,Named Entity Recognition (NER),Named Entity Recognition (NER),BC5CDR,SciBERT (SciVocab),F1,87.12,88.94,87.8,High,89.64,High,4
Natural Language Processing,Named Entity Recognition (NER),Named Entity Recognition (NER),Long-tail emerging entities,Flair embeddings,F1,40.78,50.2,41.02,Low,50.51,High,2
Natural Language Processing,Named Entity Recognition (NER),Named Entity Recognition (NER),SciERC,SciBERT (SciVocab),F1,64.2,65.5,64.66,High,65.96,High,4
Natural Language Processing,Named Entity Recognition (NER),Named Entity Recognition (NER),NCBI-disease,SciBERT (Base Vocab),F1,86.91,86.91,87.59,High,87.59,High,4
Natural Language Processing,Text Generation,Text Generation,EMNLP2017 WMT,LeakGAN,BLEU-2,0.86,0.96,0.72,Low,0.78,Low,1
Natural Language Processing,Text Generation,Text Generation,COCO Captions,LeakGAN,BLEU-2,0.83,0.95,0.69,Low,0.77,Low,1
Natural Language Processing,Text Generation,Text Generation,Chinese Poems,LeakGAN,BLEU-2,0.74,0.88,0.6,Low,0.7,Low,1
Natural Language Processing,Text Generation,Text Generation,CMU-SE,STWGAN-GP,BLEU-3,0.62,0.62,0.47,Low,0.43,Low,1
Natural Language Processing,Text Generation,Text Generation,DailyDialog,AEM+Attention,BLEU-1,14.17,14.17,14.15,Low,14.12,Low,1
Natural Language Processing,Text Generation,Text Generation,LDC2016E25,Graph2Seq,BLEU,22,22,22.06,Low,22.03,Low,1
Natural Language Processing,Dependency Parsing,Dependency Parsing,Penn Treebank,CVT + Multi-Task,UAS,93.99,96.61,94.74,High,97.38,High,4
Natural Language Processing,Dependency Parsing,Dependency Parsing,GENIA - LAS,BiLSTM-CRF,F1,91.92,91.92,92.65,High,92.65,High,4
Natural Language Processing,Dependency Parsing,Dependency Parsing,GENIA - UAS,BiLSTM-CRF,F1,92.84,92.84,93.58,High,93.58,High,4
Natural Language Processing,Relation Extraction,Relation Extraction,TACRED,TRE,F1,65.1,67.4,65.57,High,67.88,High,4
Natural Language Processing,Relation Extraction,Relation Extraction,ChemProt,SciBERT (SciVocab),F1,76.12,76.12,76.7,High,76.69,High,4
Natural Language Processing,Relation Extraction,Relation Extraction,SciERC,SciBERT (SciVocab),F1,74.64,74.64,75.2,High,75.19,High,4
Natural Language Processing,Semantic Textual Similarity,Semantic Textual Similarity,SentEval,GenSen,MRPC,0.91,0.93,0.77,Low,0.75,Low,1
Natural Language Processing,Semantic Textual Similarity,Semantic Textual Similarity,STS Benchmark,USE_T,Pearson Correlation,0.78,0.78,0.64,Low,0.6,Low,1
Natural Language Processing,Part-Of-Speech Tagging,Part-Of-Speech Tagging,Penn Treebank,Meta BiLSTM,Accuracy,97.78,97.96,98.57,High,98.75,High,4
Natural Language Processing,Part-Of-Speech Tagging,Part-Of-Speech Tagging,UD,Adversarial Bi-LSTM,Avg accuracy,96.4,96.73,97.17,High,97.51,High,4
Natural Language Processing,Part-Of-Speech Tagging,Part-Of-Speech Tagging,Social media,GATE,Accuracy,,,,Low,,Low,1
Natural Language Processing,Semantic Parsing,Semantic Parsing,spider,Exact Set Matching,Accuracy,19.7,19.7,19.74,Low,19.71,Low,1
Natural Language Processing,Text Classification,Document Classification,WOS-11967,RMDL,Accuracy,86.07,91.59,86.74,High,92.31,High,4
Natural Language Processing,Text Classification,Document Classification,WOS-46985,RMDL,Accuracy,76.58,90.69,77.16,High,91.4,High,4
Natural Language Processing,Text Classification,Document Classification,WOS-5736,RMDL,Accuracy,90.93,93.57,91.65,High,94.31,High,4
Natural Language Processing,Text Classification,Document Classification,Reuters-21578,RMDL,Accuracy,90.69,90.69,91.41,High,91.4,High,4
Natural Language Processing,Text Classification,Document Classification,IMDb,RMDL,Accuracy,90.79,90.79,91.51,High,91.51,High,4
Natural Language Processing,Text Classification,Document Classification,20NEWS,RMDL,Accuracy,87.91,87.91,88.6,High,88.6,High,4
Natural Language Processing,Semantic Role Labeling,Semantic Role Labeling,OntoNotes,BiLSTM-Span (Ensemble),F1,81.7,87,82.33,High,87.68,High,4
Natural Language Processing,Semantic Role Labeling,Semantic Role Labeling,CoNLL 2005,BiLSTM-Span (Ensemble),F1,86.04,88.5,86.71,High,89.19,High,4
Natural Language Processing,Coreference Resolution,Coreference Resolution,CoNLL 2012,"(Lee et al., 2017)+ELMo",Avg F1,67.2,73,67.69,High,73.54,High,4
Natural Language Processing,Text Summarization,Text Summarization,GigaWord,FTSum_g,ROUGE-1,36.4,37.27,36.6,Low,37.45,Low,1
Natural Language Processing,Text Summarization,Text Summarization,DUC 2004 Task 1,EndDec+WFE,ROUGE-1,28.61,32.28,28.73,Low,32.41,Low,1
Natural Language Processing,Word Sense Disambiguation,Word Sense Disambiguation,SensEval 2,"SemCor+WNGT, vocabulary reduced, ensemble",F1,74.4,75.15,74.96,High,75.71,High,4
Natural Language Processing,Word Sense Disambiguation,Word Sense Disambiguation,SemEval 2013 Task 12,"SemCor+WNGT, vocabulary reduced, ensemble",F1,69.5,72.63,70.02,High,73.16,High,4
Natural Language Processing,Word Sense Disambiguation,Word Sense Disambiguation,SensEval 3 Task 1,"LSTMLP (T:SemCor, U:1K)",F1,71.8,71.8,72.34,High,72.33,High,4
Natural Language Processing,Word Sense Disambiguation,Word Sense Disambiguation,SemEval 2007 Task 7,"SemCor+WNGT, vocabulary reduced, ensemble",F1,84.3,86.02,84.96,High,86.69,High,4
Natural Language Processing,Word Sense Disambiguation,Word Sense Disambiguation,SemEval 2007 Task 17,"SemCor+WNGT, vocabulary reduced, ensemble",F1,64.2,66.81,64.66,High,67.29,High,4
Natural Language Processing,Word Sense Disambiguation,Word Sense Disambiguation,Knowledge-based:,WSD-TM,All,66.9,66.9,67.39,High,67.38,High,4
Natural Language Processing,Word Sense Disambiguation,Word Sense Disambiguation,SemEval 2015 Task 13,"SemCor+WNGT, vocabulary reduced, ensemble",F1,72.6,74.46,73.14,High,75.01,High,4
Natural Language Processing,Entity Linking,Entity Linking,WebQSP-WD,VCG,F1,0.73,0.73,0.59,Low,0.55,Low,1
Natural Language Processing,Text Summarization,Document Summarization,CNN / Daily Mail,BERTSUM+Transformer,ROUGE-1,31.1,43.25,31.25,Low,43.49,Low,1
Natural Language Processing,Relation Classification,Relation Classification,SemEval-2010 Task 8,TRE,F1,87.1,87.1,87.78,High,87.78,High,4
Natural Language Processing,Text Classification,Sentence Classification,SciCite,SciBERT,F1,77.2,84.9,77.79,High,85.56,High,4
Natural Language Processing,Text Classification,Sentence Classification,ACL-ARC,Structural-scaffolds,F1,53,67.9,53.36,High,68.39,High,4
Natural Language Processing,Text Classification,Sentence Classification,PubMed 20k RCT,Hierarchical Neural Networks,F1,92.6,92.6,93.34,High,93.33,High,4
Natural Language Processing,Text Classification,Sentence Classification,Paper Field,SciBERT (SciVocab),F1,64.07,64.07,64.53,High,64.52,High,4
Natural Language Processing,Text Classification,Sentence Classification,ScienceCite,SciBERT (SciVocab),F1,84.99,84.99,85.65,High,85.65,High,4
Natural Language Processing,Dialogue,Dialogue Generation,Ubuntu Dialogue (Activity),MrRNN Act.-Ent.,F1,11.43,11.43,11.39,Low,11.35,Low,1
Natural Language Processing,Dialogue,Dialogue Generation,Ubuntu Dialogue (Entity),MrRNN Act.-Ent.,F1,3.72,3.72,3.6,Low,3.57,Low,1
Natural Language Processing,Dialogue,Dialogue Generation,Ubuntu Dialogue (Tense),MrRNN Act.-Ent.,Accuracy,0.29,0.29,0.14,Low,0.1,Low,1
Natural Language Processing,Dialogue,Dialogue Generation,Ubuntu Dialogue (Cmd),MrRNN Act.-Ent.,Accuracy,0.95,0.95,0.81,Low,0.77,Low,1
Natural Language Processing,Dialogue,Dialogue Generation,Twitter Dialogue (Noun),MrRNN Act.-Ent.,F1,4.63,4.63,4.52,Low,4.48,Low,1
Natural Language Processing,Dialogue,Dialogue Generation,Twitter Dialogue (Tense),MrRNN Act.-Ent.,Accuracy,0.34,0.34,0.19,Low,0.15,Low,1
Natural Language Processing,Chunking,Chunking,Penn Treebank,Flair embeddings,F1 score,95.57,96.72,96.34,High,97.5,High,4
Natural Language Processing,Paraphrase Identification,Paraphrase Identification,Quora Question Pairs,MT-DNN,Accuracy,88.17,89.6,88.86,High,90.3,High,4
Natural Language Processing,Sentiment Analysis,Aspect-Based Sentiment Analysis,SemEval 2014 Task 4 Sub Task 2,HAPN,Restaurant (Acc),75.63,82.23,76.2,High,82.86,High,4
Natural Language Processing,Sentiment Analysis,Aspect-Based Sentiment Analysis,Sentihood,Liu et al.,Aspect,69.3,78.5,69.81,High,79.09,High,4
Natural Language Processing,Constituency Parsing,Constituency Parsing,Penn Treebank,CNN Large + fine-tune,F1 score,92.1,95.6,92.83,High,96.36,High,4
Natural Language Processing,Language Acquisition,Language Acquisition,SLAM 2018,Context Based Model,AUC,0.82,0.82,0.68,Low,0.64,Low,1
Natural Language Processing,Ad-Hoc Information Retrieval,Ad-Hoc Information Retrieval,TREC Robust04,Anserini BM25+RM3,MAP,0.28,0.3,0.13,Low,0.11,Low,1
Natural Language Processing,Chinese,Chinese Word Segmentation,MSRA,Pre-trained+bigram+ LSTM+CRF,F1,97.4,97.4,98.18,High,98.18,High,4
Natural Language Processing,Grammatical Error Correction,Grammatical Error Correction,Restricted,SMT + BiGRU,F0.5,54.79,72.04,55.16,High,72.57,High,4
Natural Language Processing,Grammatical Error Correction,Grammatical Error Correction,Unrestricted,CNN Seq2Seq + Fluency Boost,F0.5,76.88,76.88,77.47,High,77.46,High,4
Natural Language Processing,Grammatical Error Correction,Grammatical Error Correction,_Restricted_,SMT + BiGRU,GLEU,57.47,61.5,57.87,High,61.92,High,4
Natural Language Processing,Question Answering,Open-Domain Question Answering,SearchQA,DecaProp,N-gram F1,22.8,70.8,22.87,Low,71.32,High,2
Natural Language Processing,Question Answering,Open-Domain Question Answering,Quasar,Denoising QA,EM (Quasar-T),26.4,42.2,26.5,Low,42.43,Low,1
Natural Language Processing,Dialogue,Dialogue State Tracking,Second dialogue state tracking challenge,StateNet,Joint,73.4,75.5,73.95,High,76.06,High,4
Natural Language Processing,Dialogue,Dialogue State Tracking,Wizard-of-Oz,StateNet,Joint,84.4,88.9,85.06,High,89.6,High,4
Natural Language Processing,Text Classification,Emotion Classification,SemEval 2018 Task 1E-c,Transformer (finetune),Macro-F1,56.1,56.1,56.49,High,56.47,High,4
Natural Language Processing,Sentiment Analysis,Multimodal Sentiment Analysis,MOSI,MMMU-BA,Accuracy,0.8,0.82,0.66,Low,0.64,Low,1
Natural Language Processing,Amr Parsing,Amr Parsing,LDC2014T12:,Transition-based+improved aligner+ensemble,F1 Newswire,0.7,0.73,0.56,Low,0.55,Low,1
Natural Language Processing,Subjectivity Analysis,Subjectivity Analysis,SUBJ,AdaSent,Accuracy,95.5,95.5,96.26,High,96.26,High,4
Natural Language Processing,Text Generation,Data-to-Text Generation,E2E NLG Challenge,S_1^R,BLEU,64.22,68.6,64.68,High,69.09,High,4
Natural Language Processing,Text Generation,Data-to-Text Generation,Rotowire (Content Selection),Neural Content Planning + conditional copy,Precision,0.29,0.34,0.14,Low,0.15,Low,1
Natural Language Processing,Text Generation,Data-to-Text Generation,RotoWire (Content Ordering),Neural Content Planning + conditional copy,DLD,0.15,0.19,0,Low,0,Low,1
Natural Language Processing,Text Generation,Data-to-Text Generation,RotoWire (Relation Generation),Neural Content Planning + conditional copy,Precision,0.75,0.87,0.61,Low,0.69,Low,1
Natural Language Processing,Text Generation,Data-to-Text Generation,RotoWire,Neural Content Planning + conditional copy,BLEU,14.19,16.5,14.17,Low,16.47,Low,1
Natural Language Processing,Text Generation,Data-to-Text Generation,SR11Deep,GCN + feat,BLEU,0.67,0.67,0.52,Low,0.48,Low,1
Natural Language Processing,Text Generation,Data-to-Text Generation,WebNLG,GCN EC,BLEU,0.56,0.56,0.41,Low,0.37,Low,1
Natural Language Processing,Sarcasm Detection,Sarcasm Detection,SARC (all-bal),CASCADE,Accuracy,75.8,77,76.38,High,77.58,High,4
Natural Language Processing,Sarcasm Detection,Sarcasm Detection,SARC (pol-bal),Bag-of-Bigrams,Accuracy,76.5,76.5,77.08,High,77.07,High,4
Natural Language Processing,Sarcasm Detection,Sarcasm Detection,SARC (pol-unbal),Bag-of-Words,Avg F1,27,27,27.11,Low,27.08,Low,1
Natural Language Processing,Stance Detection,Stance Detection,RumourEval,Kochkina et al. 2017,Accuracy,0.78,0.78,0.64,Low,0.6,Low,1
Natural Language Processing,Sentence Embeddings,Sentence Compression,Google Dataset,BiLSTM,CR,0.43,0.43,0.28,Low,0.24,Low,1
Natural Language Processing,Machine Translation,Unsupervised Machine Translation,WMT2014 French-English,SMT + NMT (tuning and joint refinement),BLEU,27.7,33.5,27.81,Low,33.64,Low,1
Natural Language Processing,Machine Translation,Unsupervised Machine Translation,WMT2016 English-German,SMT + NMT (tuning and joint refinement),BLEU,20.2,26.9,20.24,Low,26.98,Low,1
Natural Language Processing,Machine Translation,Unsupervised Machine Translation,WMT2016 German-English,SMT + NMT (tuning and joint refinement),BLEU,25.2,34.4,25.29,Low,34.55,Low,1
Natural Language Processing,Machine Translation,Unsupervised Machine Translation,WMT2014 English-French,SMT + NMT (tuning and joint refinement),BLEU,27.6,36.2,27.71,Low,36.37,Low,1
Natural Language Processing,Machine Translation,Unsupervised Machine Translation,WMT2014 German-English,SMT + NMT (tuning and joint refinement),BLEU,20.4,27,20.44,Low,27.08,Low,1
Natural Language Processing,Machine Translation,Unsupervised Machine Translation,WMT2014 English-German,SMT + NMT (tuning and joint refinement),BLEU,17,22.5,17.01,Low,22.53,Low,1
Natural Language Processing,Machine Translation,Unsupervised Machine Translation,WMT2016 Romanian-English,MLM pretraining for encoder and decoder,BLEU,31.8,31.8,31.95,Low,31.93,Low,1
Natural Language Processing,Information Extraction,Temporal Information Extraction,TimeBank,Catena,F1 score,0.51,0.51,0.36,Low,0.32,Low,1
Natural Language Processing,Information Extraction,Temporal Information Extraction,TempEval-3,Ning et al.,Temporal awareness,67.2,67.2,67.69,High,67.68,High,4
Natural Language Processing,Graph-to-Sequence,Graph-to-Sequence,LDC2015E86:,GCNSEQ,BLEU,23.95,23.95,24.03,Low,24,Low,1
Natural Language Processing,Cross-Lingual,Cross-Lingual Document Classification,MLDoc Zero-Shot English-to-Spanish,Massively Multilingual Sentence Embeddings,Accuracy,0.73,0.77,0.59,Low,0.59,Low,1
Natural Language Processing,Cross-Lingual,Cross-Lingual Document Classification,MLDoc Zero-Shot English-to-French,Massively Multilingual Sentence Embeddings,Accuracy,0.75,0.78,0.61,Low,0.6,Low,1
Natural Language Processing,Cross-Lingual,Cross-Lingual Document Classification,MLDoc Zero-Shot English-to-German,Massively Multilingual Sentence Embeddings,Accuracy,0.81,0.85,0.67,Low,0.67,Low,1
Natural Language Processing,Text Summarization,Extractive Document Summarization,CNN / Daily Mail,BERTSUM,ROUGE-2,20.24,20.24,20.28,Low,20.25,Low,1
Natural Language Processing,Grammatical Error Correction,Grammatical Error Detection,CoNLL-2014 A1,Bi-LSTM + POS (unrestricted data),F0.5,34.3,36.1,34.48,Low,36.27,Low,1
Natural Language Processing,Grammatical Error Correction,Grammatical Error Detection,CoNLL-2014 A2,Bi-LSTM + POS (unrestricted data),F0.5,44,45.1,44.27,Low,45.36,Low,1
Natural Language Processing,Grammatical Error Correction,Grammatical Error Detection,FCE,BiLSTM-JOINT,F0.5,41.1,52.07,41.34,Low,52.4,High,2
Natural Language Processing,Grammatical Error Correction,Grammatical Error Detection,JFLEG,BiLSTM-JOINT (trained on FCE),F0.5,52.52,52.52,52.87,High,52.85,High,1
Natural Language Processing,Natural Language Inference,Cross-Lingual Natural Language Inference,XNLI Zero-Shot English-to-Spanish,BERT,Accuracy,0.69,0.74,0.55,Low,0.56,Low,1
Natural Language Processing,Natural Language Inference,Cross-Lingual Natural Language Inference,XNLI Zero-Shot English-to-German,BiLSTM,Accuracy,0.68,0.73,0.54,Low,0.55,Low,1
Natural Language Processing,Natural Language Inference,Cross-Lingual Natural Language Inference,XNLI Zero-Shot English-to-French,BiLSTM,Accuracy,0.68,0.72,0.54,Low,0.54,Low,1
Natural Language Processing,Text Classification,Citation Intent Classification,ACL-ARC,Structural-scaffolds,F1,41,67.9,41.24,Low,68.39,High,2
Natural Language Processing,Text Classification,Citation Intent Classification,SciCite,SciBERT,F1,79.6,84.99,80.21,High,85.65,High,4
Natural Language Processing,Question Answering,Knowledge Base Question Answering,WebQSP-WD,GGNN,Avg F1,0.26,0.26,0.11,Low,0.07,Low,1
Natural Language Processing,Relationship Extraction (Distant Supervised),Relationship Extraction (Distant Supervised),New York Times Corpus,RESIDE,P@10%,69.4,73.6,69.91,High,74.14,High,4
Natural Language Processing,CCG Supertagging,CCG Supertagging,CCGBank,Clark et al.,Accuracy,94.7,96.1,95.46,High,96.87,High,4
Natural Language Processing,Text Generation,Table-to-text Generation,WikiBio,Field-gating Seq2seq + dual attention,BLEU,34.7,44.89,34.88,Low,45.15,Low,1
Natural Language Processing,Passage Re-Ranking,Passage Re-Ranking,MS MARCO,BERT + Small Training,MRR,0.36,0.36,0.21,Low,0.17,Low,1
Natural Language Processing,Named Entity Recognition (NER),Chinese Named Entity Recognition,Weibo NER,Lattice,F1,58.79,58.79,59.2,High,59.19,High,4
Natural Language Processing,Named Entity Recognition (NER),Chinese Named Entity Recognition,MSRA,Lattice,F1,93.18,93.18,93.92,High,93.92,High,4
Natural Language Processing,Named Entity Recognition (NER),Chinese Named Entity Recognition,OntoNotes 4,Lattice,F1,73.88,73.88,74.44,High,74.43,High,4
Natural Language Processing,Named Entity Recognition (NER),Chinese Named Entity Recognition,Resume NER,Lattice,F1,94.46,94.46,95.21,High,95.21,High,4
Natural Language Processing,Named Entity Recognition (NER),Chinese Named Entity Recognition,SighanNER,BiLSTM+CRF+adversarial+self-attention,F1,90.64,90.64,91.36,High,91.35,High,4
Natural Language Processing,Emotion Recognition,Emotion Recognition in Conversation,IEMOCAP,DialogueRNN,F1,0.56,0.65,0.41,Low,0.46,Low,1
Natural Language Processing,Hypernym Discovery,Hypernym Discovery,General,CRIM,MAP,10.6,19.78,10.55,Low,19.79,Low,1
Natural Language Processing,Hypernym Discovery,Hypernym Discovery,Medical domain,CRIM,MAP,18.84,34.05,18.87,Low,34.2,Low,1
Natural Language Processing,Hypernym Discovery,Hypernym Discovery,Music domain,CRIM,MAP,12.99,40.97,12.96,Low,41.19,Low,1
Natural Language Processing,Cross-Lingual Bitext Mining,Cross-Lingual Bitext Mining,BUCC German-to-English,Massively Multilingual Sentence Embeddings,F1 score,76.9,96.19,77.49,High,96.96,High,4
Natural Language Processing,Cross-Lingual Bitext Mining,Cross-Lingual Bitext Mining,BUCC French-to-English,Massively Multilingual Sentence Embeddings,F1 score,75.8,93.91,76.38,High,94.66,High,4
Natural Language Processing,Semantic Role Labeling,Predicate Detection,CoNLL 2005,LISA,F1,96.4,98.4,97.17,High,99.19,High,4
Natural Language Processing,Semantic Role Labeling,Predicate Detection,CoNLL 2012,LISA,F1,97.2,97.2,97.98,High,97.98,High,4
Natural Language Processing,Dialogue,Dialogue Act Classification,Switchboard corpus,CRF-ASN,Accuracy,79.2,81.3,79.81,High,81.92,High,4
Natural Language Processing,Dialogue,Dialogue Act Classification,ICSI Meeting Recorder Dialog Act (MRDA) corpus,CRF-ASN,Accuracy,90.9,91.7,91.62,High,92.43,High,4
Natural Language Processing,Entity Resolution,Entity Resolution,CoNLL 2003 (English),deep joint entity disambiguation w/ neural attention,Accuracy,92.22,92.22,92.95,High,92.95,High,4
Natural Language Processing,Semantic Role Labeling,Semantic Role Labeling (predicted predicates),CoNLL 2005,LISA + ELMo,F1,86.9,86.9,87.58,High,87.58,High,4
Natural Language Processing,Semantic Role Labeling,Semantic Role Labeling (predicted predicates),CoNLL 2012,LISA + ELMo,F1,83.38,83.38,84.03,High,84.02,High,4
Natural Language Processing,Anaphora Resolution,Abstract Anaphora Resolution,The ARRAU Corpus,MR-LSTM,Average Precision,43.83,43.83,44.1,Low,44.08,Low,1
Natural Language Processing,Query Wellformedness,Query Wellformedness,Query Wellformedness,"word-1, 2 POS-1, 2, 3",Accuracy,70.7,70.7,71.23,High,71.22,High,4
Natural Language Processing,Lexical Normalization,Lexical Normalization,LexNorm,MoNoise,Accuracy,87.63,87.63,88.32,High,88.31,High,4
,,,,,,,,44.99,,48.17,,
